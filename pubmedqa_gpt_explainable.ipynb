{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1882468-cf52-489e-9d69-8ae07d8a7802",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pandas datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42743205-58b4-42b0-a1ee-cf65f80c8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c8fc6ae5-0eb5-44c8-a70d-d6eb29be309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5093f0e8-ab8f-4a55-9c96-2426283698f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "# Get your API key securely\n",
    "api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = openai.OpenAI(api_key=api_key)  # Create a client instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5cd8dd5-05a5-4a74-8d60-e2e196b68a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load pubmedqa dataset (we'll use the 'pqa_labeled' subset)\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Select a smaller test subset for demonstration\n",
    "examples = dataset['train'].select(range(10))  # You can increase this later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1580048-7e77-4f83-93a1-46902abe455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_rows = df[df['QUESTION'].str.contains(\"Is eligibility for a chemotherapy protocol a good prognostic factor for invasive bladder cancer after radical cystectomy?\", case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab511e0-36d0-409a-902f-b3407e98ef8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59d34010-2786-4c0e-ad08-6d95b3c89ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUB_ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             16418930\n",
      "QUESTION                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "CONTEXT        {'contexts': ['Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.', '100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.', 'Differences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.'], 'labels': ['BACKGROUND', 'PATIENTS AND METHODS', 'RESULTS'], 'meshes': ['Adolescent', 'Adult', 'Aged', 'Aged, 80 and over', 'Amblyopia', 'Cataract', 'Child', 'Eye Diseases', 'Female', 'Humans', 'Male', 'Middle Aged', 'Reference Values', 'Refractive Errors', 'Reproducibility of Results', 'Retinal Diseases', 'Strabismus', 'Vision Tests', 'Visual Acuity'], 'reasoning_required_pred': ['n', 'o'], 'reasoning_free_pred': ['n', 'o']}\n",
      "ANSWER                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   no\n",
      "LONG_ANSWER                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Using the charts described, there was only a slight overestimation of visual acuity by the Snellen E compared to the Landolt C, even in strabismus amblyopia. Small differences in the lower visual acuity range have to be considered.\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "df = pd.DataFrame({\n",
    "    'PUB_ID': examples['pubid'],\n",
    "    'QUESTION': examples['question'],\n",
    "    'CONTEXT': examples['context'],\n",
    "    'ANSWER': examples['final_decision'],  # Ground truth: 'yes', 'no', or 'maybe'\n",
    "    'LONG_ANSWER': examples['long_answer']\n",
    "})\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(df.loc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "836bd369-7179-4bff-a6ef-a8baafe1469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt(question, context):\n",
    "    prompt = (\n",
    "        f\"You are a biomedical expert assistant. Given the following abstract:\\n\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Please answer with 'yes', 'no', or 'maybe' and explain your reasoning.\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=120,\n",
    "        n=1,\n",
    "    presence_penalty=0.0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "96e916a7-2e9b-44e3-8fc5-a8d363744769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(gpt_output):\n",
    "    match = re.search(r\"\\b(yes|no|maybe)\\b\", gpt_output.lower())\n",
    "    return match.group(1) if match else \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74f14757-0f1a-4072-b38a-76e0cf62ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question 1...\n",
      "Processing question 2...\n",
      "Processing question 3...\n",
      "Processing question 4...\n",
      "Processing question 5...\n",
      "Processing question 6...\n",
      "Processing question 7...\n",
      "Processing question 8...\n",
      "Processing question 9...\n",
      "Processing question 10...\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Processing question {i+1}...\")\n",
    "    gpt_response = ask_gpt(row['QUESTION'], row['CONTEXT'])\n",
    "    predicted = extract_answer(gpt_response)\n",
    "\n",
    "    results.append({\n",
    "        'question': row['QUESTION'],\n",
    "        'ground_truth': row['ANSWER'],\n",
    "        'predicted_answer': predicted,\n",
    "        'gpt_explanation': gpt_response,\n",
    "         'reference_explanation': row['CONTEXT']  # <-- add this line\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "06eda978-fa99-49f6-b5a7-c0e81e3301fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
      "Ground Truth Answer: yes\n",
      "Predicted Answer: yes\n",
      "GPT Explanation: Yes, the abstract suggests that mitochondria play a role in the remodeling of lace plant leaves during programmed cell death. The study found that mitochondrial dynamics, including distribution, motility, and membrane potential, change during different stages of programmed cell death in the lace plant. Furthermore, treatment with cyclosporine A, which indirectly affects mitochondrial permeability transition pore formation, resulted in lace plant leaves with significantly fewer perforations, suggesting that mitochondria play a role in this process.\n",
      "Reference Explanation (Abstract): {'contexts': ['Programmed cell death (PCD) is the regulated death of cells within an organism. The lace plant (Aponogeton madagascariensis) produces perforations in its leaves through PCD. The leaves of the plant consist of a latticework of longitudinal and transverse veins enclosing areoles. PCD occurs in the cells at the center of these areoles and progresses outwards, stopping approximately five cells from the vasculature. The role of mitochondria during PCD has been recognized in animals; however, it has been less studied during PCD in plants.', 'The following paper elucidates the role of mitochondrial dynamics during developmentally regulated PCD in vivo in A. madagascariensis. A single areole within a window stage leaf (PCD is occurring) was divided into three areas based on the progression of PCD; cells that will not undergo PCD (NPCD), cells in early stages of PCD (EPCD), and cells in late stages of PCD (LPCD). Window stage leaves were stained with the mitochondrial dye MitoTracker Red CMXRos and examined. Mitochondrial dynamics were delineated into four categories (M1-M4) based on characteristics including distribution, motility, and membrane potential (ΔΨm). A TUNEL assay showed fragmented nDNA in a gradient over these mitochondrial stages. Chloroplasts and transvacuolar strands were also examined using live cell imaging. The possible importance of mitochondrial permeability transition pore (PTP) formation during PCD was indirectly examined via in vivo cyclosporine A (CsA) treatment. This treatment resulted in lace plant leaves with a significantly lower number of perforations compared to controls, and that displayed mitochondrial dynamics similar to that of non-PCD cells.'], 'labels': ['BACKGROUND', 'RESULTS'], 'meshes': ['Alismataceae', 'Apoptosis', 'Cell Differentiation', 'Mitochondria', 'Plant Leaves'], 'reasoning_required_pred': ['y', 'e', 's'], 'reasoning_free_pred': ['y', 'e', 's']}\n"
     ]
    }
   ],
   "source": [
    "data_point = results[0]\n",
    "\n",
    "print(\"Question:\", data_point['question'])\n",
    "print(\"Ground Truth Answer:\", data_point['ground_truth'])\n",
    "print(\"Predicted Answer:\", data_point['predicted_answer'])\n",
    "print(\"GPT Explanation:\", data_point['gpt_explanation'])\n",
    "print(\"Reference Explanation (Abstract):\", data_point['reference_explanation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d3e702d-8382-4eb4-80c7-dbbe7b9e06db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on 10 examples: 50.00%\n",
      "Saved results to gpt4_pubmedqa_results_05192025.csv\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gpt4_pubmedqa_results_05192025.csv\", index=False)\n",
    "\n",
    "accuracy = (results_df['ground_truth'].str.lower() == results_df['predicted_answer']).mean()\n",
    "print(f\"\\nAccuracy on {len(results_df)} examples: {accuracy*100:.2f}%\")\n",
    "print(\"Saved results to gpt4_pubmedqa_results_05192025.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7f63f38-5e05-4744-b45f-0387e2100057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "#EXACT MATCH\n",
    "\n",
    "correct = 0\n",
    "total = len(results)\n",
    "\n",
    "for r in results:\n",
    "    if r['predicted_answer'].strip().lower() == r['ground_truth'].strip().lower():\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Exact Match Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ca0ee-fb34-4b39-a1b9-209632cfc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00841f-82be-40a8-995f-a153f3fb6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Extract lists of explanations\n",
    "gpt_explanations = [r['gpt_explanation'] for r in results]\n",
    "reference_explanations = [r['reference_explanation'] for r in results]\n",
    "\n",
    "P, R, F1 = score(gpt_explanations, reference_explanations, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "print(f\"BERTScore F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203739b-5428-4d5a-8884-1ba238314cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Extract lists of explanations\n",
    "gpt_explanations = [r['gpt_explanation'] for r in results]\n",
    "reference_explanations = [r['reference_explanation'] for r in results]\n",
    "\n",
    "P, R, F1 = score(gpt_explanations, reference_explanations, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "print(f\"BERTScore F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7ec51-e655-4376-967a-4a6b7036e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9894c63-a6ca-44fc-b877-3cf76a17ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score\n",
    "# from bart_score import BARTScorer\n",
    "\n",
    "# Extract lists of predictions and references\n",
    "predictions = [r['gpt_explanation'] for r in results]\n",
    "references = [r['reference_explanation'] for r in results]  # You need to have this in your data\n",
    "\n",
    "# -------- BERTScore --------\n",
    "# P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", verbose=True)\n",
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang=\"en\",  # keep lang as 'en'\n",
    "    model_type=\"bert-base-uncased\",  # Use this instead\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Average BERTScore F1\n",
    "bert_f1_avg = F1.mean().item()\n",
    "print(f\"Average BERTScore F1: {bert_f1_avg:.4f}\")\n",
    "\n",
    "# -------- BARTScore --------\n",
    "# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "# Compute scores\n",
    "# bart_scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "# bart_avg = sum(bart_scores) / len(bart_scores)\n",
    "# print(f\"Average BARTScore: {bart_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec8e4c-1084-479d-bbc0-d1f64d8ed846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/neulab/BARTScore.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19b3f0ff-7a38-48f1-8d7a-657013194200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnpri\\ACP_GenAI_June\\BARTScore\n"
     ]
    }
   ],
   "source": [
    "cd BARTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7aa27-7b02-4c4e-a6d4-7b1a6ce13c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [str(r['gpt_explanation']) for r in results if r['gpt_explanation'] is not None]\n",
    "references = [str(r['reference_explanation']) for r in results if r['reference_explanation'] is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d3b66-f173-4609-b304-5935556ee9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_score import BARTScorer\n",
    "print(\"ppp\")\n",
    "# Load the BARTScorer with a suitable model\n",
    "bart_scorer = BARTScorer(device='cpu',checkpoint='facebook/bart-base')\n",
    "print(\"loaded\")\n",
    "# Compute BARTScores\n",
    "scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "average_score = sum(scores) / len(scores)\n",
    "\n",
    "print(\"Average BARTScore:\", average_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547516a-38d4-42f8-accd-3cfe1416b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask GPT with Biomedical Sentence Highlighting\n",
    "\n",
    "def ask_gpt_highlight(question, context):\n",
    "    prompt = (\n",
    "        f\"You are a biomedical expert. Given the abstract below, and the question:\\n\\n\"\n",
    "        f\"--- ABSTRACT START ---\\n{context}\\n--- ABSTRACT END ---\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Please answer with one of: 'yes', 'no', or 'maybe'.\\n\"\n",
    "        f\"Then explain your reasoning in 1-2 sentences.\\n\"\n",
    "        f\"Finally, list 1-3 sentences from the abstract that best support your answer.\\n\\n\"\n",
    "        f\"Format:\\nAnswer: <yes/no/maybe>\\nExplanation: <your explanation>\\nSupporting Sentences: <copied sentences from abstract>\\n\\n\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce48ec8-20c0-49fb-8c4d-dd66c07759bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b63e9-733a-4136-8768-4dc7c574f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Processing question {i+1}...\")\n",
    "    gpt_response = ask_gpt_highlight(row['QUESTION'], row['CONTEXT'])\n",
    "\n",
    "    # Extract components using regex\n",
    "    answer_match = re.search(r\"(?i)Answer:\\s*(yes|no|maybe)\", gpt_response)\n",
    "    explanation_match = re.search(r\"(?i)Explanation:\\s*(.+?)\\n(?:Supporting Sentences|$)\", gpt_response, re.DOTALL)\n",
    "    support_match = re.search(r\"(?i)Supporting Sentences:\\s*(.+)\", gpt_response, re.DOTALL)\n",
    "\n",
    "    predicted = answer_match.group(1).lower() if answer_match else \"unknown\"\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "    support = support_match.group(1).strip() if support_match else \"\"\n",
    "\n",
    "    results.append({\n",
    "        'question': row['QUESTION'],\n",
    "        'ground_truth': row['ANSWER'],\n",
    "        'predicted_answer': predicted,\n",
    "        'gpt_explanation': explanation,\n",
    "        'supporting_sentences': support\n",
    "    })\n",
    "\n",
    "# Save and evaluate\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gpt4_pubmedqa_explained_highlighted.csv\", index=False)\n",
    "\n",
    "accuracy = (results_df['ground_truth'].str.lower() == results_df['predicted_answer']).mean()\n",
    "print(f\"\\nAccuracy on {len(results_df)} examples: {accuracy*100:.2f}%\")\n",
    "print(\"Saved results to gpt4_pubmedqa_explained_highlighted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ecb446-1690-4cf8-9c84-f558eba68e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      Great choice! We can use SBERT (Sentence-BERT) to compute semantic similarity between:\n",
    "\n",
    "# Sentences from the abstract, and\n",
    "\n",
    "# The supporting sentences identified by GPT-4.\n",
    "\n",
    "# This allows you to verify whether GPT-4 actually picked the most relevant evidence from the abstract — a strong step toward explainability and trust.\n",
    "\n",
    "# ✅ Overview of the SBERT-based Cross-Check Process\n",
    "# 🧩 Components:\n",
    "# Break abstract into sentences (context_sents)\n",
    "\n",
    "# Extract GPT's claimed supporting sentences (gpt_support)\n",
    "\n",
    "# Compute SBERT embeddings for both\n",
    "\n",
    "# Compute cosine similarity\n",
    "\n",
    "# Find top N sentences closest to GPT's selections\n",
    "\n",
    "# Report whether they match the claimed support\n",
    "\n",
    "# ✅ Installation (Anaconda Terminal)\n",
    "# bash\n",
    "# Copy\n",
    "# Edit\n",
    "# pip install sentence-transformers scikit-learn\n",
    "# ✅ Add This to Your Script\n",
    "# 🔹 Step 1: SBERT Setup\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # Load SBERT model (biomed model available too)\n",
    "# sbert_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and general\n",
    "# 🔹 Step 2: Cross-Check Function\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# def cross_check_similarity(context, gpt_support):\n",
    "#     # Split context and GPT supporting sentences\n",
    "#     context_sents = sent_tokenize(context)\n",
    "#     gpt_sents = sent_tokenize(gpt_support)\n",
    "\n",
    "#     # Get embeddings\n",
    "#     context_emb = sbert_model.encode(context_sents, convert_to_tensor=True)\n",
    "#     gpt_emb = sbert_model.encode(gpt_sents, convert_to_tensor=True)\n",
    "\n",
    "#     # For each GPT-support sentence, find top match from context\n",
    "#     match_results = []\n",
    "#     for i, gpt_vector in enumerate(gpt_emb):\n",
    "#         cos_sim = util.pytorch_cos_sim(gpt_vector, context_emb)[0]\n",
    "#         best_idx = cos_sim.argmax().item()\n",
    "#         similarity_score = cos_sim[best_idx].item()\n",
    "#         match_results.append({\n",
    "#             \"gpt_sentence\": gpt_sents[i],\n",
    "#             \"matched_context_sentence\": context_sents[best_idx],\n",
    "#             \"similarity_score\": round(similarity_score, 3)\n",
    "#         })\n",
    "\n",
    "#     return match_results\n",
    "# 🔹 Step 3: Run SBERT Cross-Check After GPT Response\n",
    "# In your main loop (after getting GPT output):\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "#     # SBERT cross-check\n",
    "#     sbert_matches = cross_check_similarity(row['context'], support)\n",
    "#     results.append({\n",
    "#         'question': row['question'],\n",
    "#         'ground_truth': row['answer'],\n",
    "#         'predicted_answer': predicted,\n",
    "#         'gpt_explanation': explanation,\n",
    "#         'supporting_sentences': support,\n",
    "#         'sbert_matched_sentences': sbert_matches\n",
    "#     })\n",
    "# If you're saving to a file, convert sbert_matched_sentences to string:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# import json\n",
    "\n",
    "# results_df['sbert_matched_sentences'] = results_df['sbert_matched_sentences'].apply(json.dumps)\n",
    "# 📊 Example Output (Console or CSV)\n",
    "# json\n",
    "# Copy\n",
    "# Edit\n",
    "# [\n",
    "#   {\n",
    "#     \"gpt_sentence\": \"Treatment X improved patient survival.\",\n",
    "#     \"matched_context_sentence\": \"Patients treated with X had significantly better survival rates.\",\n",
    "#     \"similarity_score\": 0.88\n",
    "#   }\n",
    "# ]\n",
    "# 🔍 Use Cases for This\n",
    "# Identify hallucinated support (GPT picked a sentence not in the abstract).\n",
    "\n",
    "# Quantify the factual alignment of explanation vs. source.\n",
    "\n",
    "# Flag weak justifications (low similarity).\n",
    "\n",
    "# Would you like to flag explanations under a similarity threshold (e.g. < 0.7) as unreliable?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# You said:\n",
    "# yes\n",
    "\n",
    "\n",
    "# ChatGPT said:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3bf7d6-3e99-4c91-9d10-3e115f74da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print(f\"Processing question {i+1}...\")\n",
    "    \n",
    "    # Extract components using regex\n",
    "    answer_match = re.search(r\"(?i)Answer:\\s*(yes|no|maybe)\", gpt_response)\n",
    "    explanation_match = re.search(r\"(?i)Explanation:\\s*(.+?)\\n(?:Supporting Sentences|$)\", gpt_response, re.DOTALL)\n",
    "    support_match = re.search(r\"(?i)Supporting Sentences:\\s*(.+)\", gpt_response, re.DOTALL)\n",
    "\n",
    "    predicted = answer_match.group(1).lower() if answer_match else \"unknown\"\n",
    "    explanation = explanation_match.group(1).strip() if explanation_match else \"\"\n",
    "    support = support_match.group(1).strip() if support_match else \"\"\n",
    "\n",
    "    # Ensure 'support' is a string if it is a list or other object\n",
    "    if isinstance(support, list):\n",
    "        support = \" \".join(support)\n",
    "\n",
    "    # SBERT cross-check\n",
    "    sbert_matches = cross_check_similarity(row['context'], support)\n",
    "\n",
    "    # Flag explanations with low similarity (< 0.7)\n",
    "    flagged_sentences = [match for match in sbert_matches if match['similarity_score'] < 0.7]\n",
    "\n",
    "    results.append({\n",
    "        'question': row['question'],\n",
    "        'ground_truth': row['answer'],\n",
    "        'predicted_answer': predicted,\n",
    "        'gpt_explanation': explanation,\n",
    "        'supporting_sentences': support,\n",
    "        'sbert_matched_sentences': sbert_matches,\n",
    "        'flagged_sentences': flagged_sentences\n",
    "    })\n",
    "\n",
    "# Save and evaluate\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gpt4_pubmedqa_explained_highlighted_flagged.csv\", index=False)\n",
    "\n",
    "accuracy = (results_df['ground_truth'].str.lower() == results_df['predicted_answer']).mean()\n",
    "print(f\"\\nAccuracy on {len(results_df)} examples: {accuracy*100:.2f}%\")\n",
    "print(\"Saved results to gpt4_pubmedqa_explained_highlighted_flagged.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da35fa0-1688-4d87-80c7-b8380b07dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Few Shot prompting technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba52726-89da-4340-84b3-2ed35282d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot prompt examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"question\": \"Does the study support the use of aspirin in preventing heart attacks?\",\n",
    "        \"context\": \"A randomized study of 20,000 participants showed a 25% reduction in heart attacks among those who took aspirin regularly compared to placebo.\",\n",
    "        \"answer\": \"Yes\",\n",
    "        \"explanation\": \"The abstract clearly states a 25% reduction in heart attacks among aspirin users, indicating a positive effect.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Is metformin effective in treating Alzheimer's disease?\",\n",
    "        \"context\": \"Although metformin has shown promise in laboratory models, clinical trials in humans have not demonstrated significant cognitive improvement.\",\n",
    "        \"answer\": \"No\",\n",
    "        \"explanation\": \"While lab results were positive, the abstract confirms that human trials did not show cognitive benefit, so the answer is no.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Does this paper conclude that the new vaccine is safe?\",\n",
    "        \"context\": \"The phase 3 clinical trial reported minimal side effects and no serious adverse events among 15,000 participants.\",\n",
    "        \"answer\": \"Yes\",\n",
    "        \"explanation\": \"The trial reported minimal side effects and no serious adverse events, suggesting the vaccine is safe.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb402a56-a7a7-4e00-9aa2-3dcc9cd2b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format few-shot prompt block\n",
    "def format_few_shot_block(examples):\n",
    "    prompt = \"\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"\"\"Abstract:\n",
    "{ex['context']}\n",
    "\n",
    "Question: {ex['question']}\n",
    "Answer: {ex['answer']}\n",
    "Explanation: {ex['explanation']}\n",
    "\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f8480-d21f-47b9-8ac8-8e22b82252b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_block = format_few_shot_block(few_shot_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a1fe0-36cc-438a-9971-ae130a264d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd0f766e-cd00-4f1f-9d93-6cf82d43f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full prompt for each instance\n",
    "def ask_gpt_few_shot(question, context):\n",
    "    full_prompt = (\n",
    "        \"You are a biomedical research assistant helping answer clinical questions from abstracts.\\n\\n\"\n",
    "        \"Below are some examples of question answering with explanations based on biomedical abstracts:\\n\\n\"\n",
    "        f\"{few_shot_block}\"\n",
    "        f\"Abstract:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "        temperature=0.0,\n",
    "        max_tokens=120,\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "de8b2414-5334-4b62-89de-4fed9945f341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Explanation: The abstract discusses a study that elucidates the role of mitochondrial dynamics during developmentally regulated PCD in the lace plant. The results suggest that mitochondrial dynamics are involved in the process of PCD in the lace plant, which is responsible for the formation of perforations in its leaves.\n"
     ]
    }
   ],
   "source": [
    "# Run on one sample\n",
    "sample = df.iloc[0]\n",
    "response = ask_gpt_few_shot(sample['QUESTION'], sample['CONTEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ff9d887-1a39-4a60-824a-07edf7da5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load pubmedqa dataset (we'll use the 'pqa_labeled' subset)\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# Select a smaller test subset for demonstration\n",
    "examples = dataset['train'].select(range(10))  # You can increase this later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fbd6f68a-fd77-4a2f-a6d9-1da24a914dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question 1...\n",
      "Yes\n",
      "Explanation: The abstract discusses a study that elucidates the role of mitochondrial dynamics during developmentally regulated PCD in the lace plant. The results suggest that mitochondrial dynamics are involved in the process of PCD in the lace plant, which is responsible for the formation of perforations in its leaves.\n",
      "Processing question 2...\n",
      "Small\n",
      "Explanation: The abstract mentions that the mean difference between Landolt C acuity (LR) and Snellen E acuity (SE) was 0.55 lines for the eyes with strabismus amblyopia, indicating only a small difference.\n",
      "Processing question 3...\n",
      "Yes\n",
      "Explanation: The abstract discusses a case where infants became pale, hypotonic, still and unreactive during bathing, which is similar to symptoms of aquagenic urticaria. The infants also showed an increase in blood histamine levels after a trial bath, which is a common reaction in urticaria. Therefore, it can be inferred that the infants might have a pediatric form of water-induced urticaria.\n",
      "Processing question 4...\n",
      "Yes and No\n",
      "Explanation: The abstract states that overall scores were similar between the two procedures. However, the continence score was significantly better in the transabdominal group, while the stool pattern score was better in the transanal group. Therefore, the results are not entirely equal, with each procedure having its own advantages.\n",
      "Processing question 5...\n",
      "Yes\n",
      "Explanation: The abstract states that compared to usual care, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule. This suggests that tailored interventions can increase mammography use.\n",
      "Processing question 6...\n",
      "Yes\n",
      "Explanation: The abstract provides data on 88 DBE procedures performed on 66 patients, with various treatments applied such as electrocautery, biopsy, and dilation of strictures. It does not mention any adverse events or complications, suggesting that DBE is both efficacious and safe in a community setting.\n",
      "Processing question 7...\n",
      "Yes\n",
      "Explanation: The abstract mentions that emergency surgery is associated with poorer outcomes and higher mortality, with 30-day mortality being 12-22% and 1-year mortality being 25-38%. This suggests a need for improvement in this area.\n",
      "Processing question 8...\n",
      "Yes\n",
      "Explanation: The study found that after adjusting for reporting heterogeneity, the factors significantly associated with sleep-related problems changed, indicating that adjustment for reporting heterogeneity is necessary.\n",
      "Processing question 9...\n",
      "No\n",
      "Explanation: The abstract states that despite significantly lower HDL cholesterol due to genetic mutations, the carotid intima-media thickness was nearly identical between cases and controls. This suggests that these mutations do not promote increased carotid intima-media thickness.\n",
      "Processing question 10...\n",
      "Yes\n",
      "Explanation: The abstract states that the short stay ward increased hospital efficiency, had a low rate of patient admission to an in-hospital bed, and had a low number of unscheduled visits within 72 hours of discharge. There were no adverse events reported and parental satisfaction was high. Additionally, the implementation of the short stay ward resulted in significant estimated savings for the hospitals.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Processing question {i+1}...\")\n",
    "    gpt_response = ask_gpt_few_shot(row['QUESTION'], row['CONTEXT'])\n",
    "    predicted = extract_answer(gpt_response)\n",
    "\n",
    "    results.append({\n",
    "        'question': row['QUESTION'],\n",
    "        'ground_truth': row['ANSWER'],\n",
    "        'predicted_answer': predicted,\n",
    "        'gpt_explanation': gpt_response,\n",
    "         'reference_explanation': row['CONTEXT']  # <-- add this line\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "68e0944b-060e-4cbd-8c08-cb6c545f8712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on 10 examples: 60.00%\n",
      "Saved results to gpt4_pubmedqa_results_few_shot.csv\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"gpt4_pubmedqa_results_few_shot.csv\", index=False)\n",
    "\n",
    "accuracy = (results_df['ground_truth'].str.lower() == results_df['predicted_answer']).mean()\n",
    "print(f\"\\nAccuracy on {len(results_df)} examples: {accuracy*100:.2f}%\")\n",
    "print(\"Saved results to gpt4_pubmedqa_results_few_shot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8f2072b1-6d41-4149-8fd0-4d5263195808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "#EXACT MATCH\n",
    "\n",
    "correct = 0\n",
    "total = len(results)\n",
    "\n",
    "for r in results:\n",
    "    if r['predicted_answer'].strip().lower() == r['ground_truth'].strip().lower():\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Exact Match Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cc731509-4fa7-444b-bb97-cd215a29e8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore F1: 0.3216\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Extract lists of explanations\n",
    "gpt_explanations = [r['gpt_explanation'] for r in results]\n",
    "reference_explanations = [r['reference_explanation'] for r in results]\n",
    "\n",
    "P, R, F1 = score(gpt_explanations, reference_explanations, lang=\"en\", model_type=\"bert-base-uncased\")\n",
    "print(f\"BERTScore F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1a6460a8-0b79-4783-bd56-3dae86c34ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bd82eb9bae42dd99d6a40eca1c5ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d55d05ac3304d66a199df406ad6c743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.47 seconds, 20.26 sentences/sec\n",
      "Average BERTScore F1: 0.3216\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score as bert_score\n",
    "# from bart_score import BARTScorer\n",
    "\n",
    "# Extract lists of predictions and references\n",
    "predictions = [r['gpt_explanation'] for r in results]\n",
    "references = [r['reference_explanation'] for r in results]  # You need to have this in your data\n",
    "\n",
    "# -------- BERTScore --------\n",
    "# P, R, F1 = bert_score(predictions, references, lang=\"en\", model_type=\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", verbose=True)\n",
    "from bert_score import score\n",
    "\n",
    "P, R, F1 = score(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang=\"en\",  # keep lang as 'en'\n",
    "    model_type=\"bert-base-uncased\",  # Use this instead\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Average BERTScore F1\n",
    "bert_f1_avg = F1.mean().item()\n",
    "print(f\"Average BERTScore F1: {bert_f1_avg:.4f}\")\n",
    "\n",
    "# -------- BARTScore --------\n",
    "# bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "# Compute scores\n",
    "# bart_scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "# bart_avg = sum(bart_scores) / len(bart_scores)\n",
    "# print(f\"Average BARTScore: {bart_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e62040e3-d0ba-494c-89e6-d656b0f44ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [str(r['gpt_explanation']) for r in results if r['gpt_explanation'] is not None]\n",
    "references = [str(r['reference_explanation']) for r in results if r['reference_explanation'] is not None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fdbb9697-2bb4-4b02-8271-79c3fc16b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppp\n",
      "loaded\n",
      "Average BARTScore: -7.9501815392420845\n"
     ]
    }
   ],
   "source": [
    "from bart_score import BARTScorer\n",
    "print(\"ppp\")\n",
    "# Load the BARTScorer with a suitable model\n",
    "bart_scorer = BARTScorer(device='cpu',checkpoint='facebook/bart-base')\n",
    "print(\"loaded\")\n",
    "# Compute BARTScores\n",
    "scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "average_score = sum(scores) / len(scores)\n",
    "\n",
    "print(\"Average BARTScore:\", average_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
